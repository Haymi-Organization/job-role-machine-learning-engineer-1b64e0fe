# Week 1, Task 2: Build and Evaluate a Text Classifier\n\n## ðŸŽ¯ Objective\nTrain a machine learning model to classify text using the features generated in Task 1, evaluate its performance, and demonstrate model persistence.\n\n## ðŸ“‹ Requirements\n1.  **Data Integration:** Create a Python script (`task2.py`) that imports and utilizes the feature extraction logic (or the saved features) from `task1.py`.\n2.  **Dataset Splitting:** Split your dataset into training and testing sets (e.g., 80% train, 20% test) using `sklearn.model_selection.train_test_split`.\n3.  **Model Training:**\n    *   **Scikit-learn:** Train a classification model such as `LogisticRegression`, `SVC`, or `MultinomialNB` from `sklearn` on your training data.\n    *   **TensorFlow (Optional/Advanced):** For a more advanced approach, you may use `tensorflow.keras` to build a simple neural network (e.g., a `Dense` layer network) for classification. This is not strictly required but demonstrates proficiency.\n4.  **Model Evaluation:** Evaluate your trained model's performance on the test set. Report standard classification metrics:\n    *   Accuracy\n    *   Precision (macro or weighted average)\n    *   Recall (macro or weighted average)\n    *   F1-Score (macro or weighted average)\n    *   **Output:** Print these metrics to the console or save them to a file.\n5.  **Model Persistence:** Save your trained model using `joblib` (for scikit-learn models) or `tensorflow.keras.models.save_model` (for Keras models). The saved model should be loadable for future predictions.\n6.  **Documentation:** Provide comments in your code and update the `README.md` in the `submissions/week-1/task-2/` directory, explaining your model choice, evaluation results, and how to run your script.\n\n## ðŸ§ª Deliverables\n*   `task2.py`\n*   `README.md`\n*   Saved model file (e.g., `model.joblib` or `model.h5`)